{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mkl\n",
    "\n",
    "mkl.set_num_threads(4)\n",
    "np.random.seed(1234)\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handy utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros(*dims):\n",
    "    return np.zeros(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def rand(*dims):\n",
    "    return np.random.rand(*dims).astype(np.float32)\n",
    "\n",
    "def chunks(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "def as_matrix(vector):\n",
    "    return np.reshape(vector, (-1, 1))\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    one_hot = zeros(labels.shape[0], np.max(labels) + 1) \n",
    "    one_hot[np.arange(labels.shape[0]), labels] = 1\n",
    "    return one_hot.astype(np.float32)\n",
    "\n",
    "def tiles(examples):\n",
    "    rows_count = examples.shape[0]\n",
    "    cols_count = examples.shape[1]\n",
    "    tile_height = examples.shape[2]\n",
    "    tile_width = examples.shape[3]\n",
    "    \n",
    "    space_between_tiles = 2\n",
    "    img_matrix = np.empty(shape=(rows_count * (tile_height + space_between_tiles) - space_between_tiles,  \n",
    "                                 cols_count * (tile_width + space_between_tiles) - space_between_tiles))\n",
    "    img_matrix.fill(np.nan)\n",
    "\n",
    "    for r in range(rows_count):\n",
    "        for c in range(cols_count):\n",
    "            x_0 = r * (tile_height + space_between_tiles)\n",
    "            y_0 = c * (tile_width + space_between_tiles)\n",
    "            ex_min = np.min(examples[r, c])\n",
    "            ex_max = np.max(examples[r, c])\n",
    "            img_matrix[x_0:x_0 + tile_height, y_0:y_0 + tile_width] = (examples[r, c] - ex_min) / (ex_max - ex_min)\n",
    "    \n",
    "    plt.matshow(img_matrix, cmap='gray', interpolation='none')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(batch):\n",
    "    return np.maximum(0.0, batch)\n",
    "\n",
    "def relu_derivative(batch):\n",
    "    return (batch > 0).astype(np.float32)\n",
    "\n",
    "def softmax(batch):\n",
    "    m = as_matrix(np.max(batch, axis=1))\n",
    "    exponents = np.exp(batch - m)\n",
    "    return exponents / as_matrix(np.sum(exponents, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 503: Service Unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3f9e2f2eef05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdigits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtiles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdigits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mnist\\__init__.py\u001b[0m in \u001b[0;36mtrain_images\u001b[1;34m()\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \"\"\"\n\u001b[1;32m--> 161\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdownload_and_parse_mnist_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train-images-idx3-ubyte.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mnist\\__init__.py\u001b[0m in \u001b[0;36mdownload_and_parse_mnist_file\u001b[1;34m(fname, target_dir, force)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mNumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdimensions\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mIDX\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \"\"\"\n\u001b[1;32m--> 143\u001b[1;33m     \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m     \u001b[0mfopen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'.gz'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mfopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mnist\\__init__.py\u001b[0m in \u001b[0;36mdownload_file\u001b[1;34m(fname, target_dir, force)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mforce\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_fname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_fname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtarget_fname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 641\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 503: Service Unavailable"
     ]
    }
   ],
   "source": [
    "import mnist\n",
    "digits = np.reshape(mnist.train_images()[:12*24], newshape=(12, 24, 28, 28))\n",
    "tiles(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network layers\n",
    "\n",
    "We define three types of layers: densely connected layer (```Dense```), convolutional layer (```Conv2D```) and non-linearity (```Nonlinear```). Each layer supports two operations:\n",
    "\n",
    "- signal propagation: ```forward```\n",
    "- error backpropagation: ```backward```\n",
    "\n",
    "```backward``` operation backpropagates the delta and updates layer weights and biases.\n",
    "\n",
    "Weights are initialized with \"variance scaling\" method (also known as \"He initialization\"). This initialization was proposed together with residual networks and is appropriate for networks with ReLU activations. Source: https://arxiv.org/pdf/1502.01852v1.pdf\n",
    "\n",
    "**Note that in this lab we have separate vectors for biases.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Nonlinear:\n",
    "    def __init__(self, activation_fun, d_activation_fun):\n",
    "        self.activation_fun = activation_fun\n",
    "        self.d_activation_fun = d_activation_fun\n",
    "        \n",
    "        self.visible = None\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        self.visible = batch\n",
    "        return self.activation_fun(batch)\n",
    "    \n",
    "    def backward(self, deltas):\n",
    "        return deltas * self.d_activation_fun(self.visible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Densly connected layer\n",
    "\n",
    "Implement missing parts in the ```backward``` method. Remember that the biases are kept in a separate vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, visible_size, hidden_size, learning_rate, momentum):\n",
    "        self.visible_size = visible_size\n",
    "        self.hidden_size = hidden_size\n",
    "                \n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.W = np.random.normal(scale=np.sqrt(2.0 / self.visible_size),\n",
    "                                  size=(self.visible_size, self.hidden_size)).astype(np.float32)\n",
    "        self.B = zeros(1, self.hidden_size)\n",
    "        \n",
    "        self.in_shape = None\n",
    "        \n",
    "        self.visible = None\n",
    "        self.hidden  = None\n",
    "\n",
    "        self.MW = zeros(self.visible_size, self.hidden_size)\n",
    "        self.MB = zeros(1, self.hidden_size)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        self.in_shape = batch.shape\n",
    "        self.visible = np.reshape(batch, (batch.shape[0], -1))\n",
    "        \n",
    "        self.hidden = self.visible @ self.W + self.B\n",
    "        return self.hidden\n",
    "    \n",
    "    def backward(self, deltas):\n",
    "        observations_count = self.visible.shape[0]\n",
    "        \n",
    "        prev_deltas = deltas @ self.W.T\n",
    "        prev_deltas = prev_deltas.reshape(self.in_shape)\n",
    "        \n",
    "        \n",
    "        raise NotImplementedError('Gradient calculation in Dense is not implemented.')\n",
    "        \n",
    "        # Calculate the gradient for the weights and biases. Store the result\n",
    "        # in `grad_w` and `grad_b`, respectively.\n",
    "        #\n",
    "        # This is a standard densly connected layer, so take hints from the backprop\n",
    "        # implementation in previous labs. Note, however, that we now have a separate\n",
    "        # vector for biases. This means that you need to deduce an explicit equation\n",
    "        # for the gradient with respect to the biases (consider how were the biases\n",
    "        # updated in the previous backprop implementation).\n",
    "        #\n",
    "        # Inputs to the current layer are stored in `self.visible`\n",
    "        # Deltas for the current layer are stored in `deltas`.\n",
    "        \n",
    "        # grad_w = ???\n",
    "        # grad_b = ???\n",
    "        \n",
    "        \n",
    "        grad_b = np.expand_dims(grad_b, axis=0)\n",
    "        \n",
    "        self.MW = self.momentum * self.MW - \\\n",
    "                  self.learning_rate / observations_count * grad_w\n",
    "        self.MB = self.momentum * self.MB - \\\n",
    "                  self.learning_rate / observations_count * grad_b\n",
    "        \n",
    "        self.W += self.MW\n",
    "        self.B += self.MB\n",
    "        \n",
    "        return prev_deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional layers\n",
    "\n",
    "We implement the convolution operation as a simple matrix multiplication. Essentially, convolution is a dot product between the kernel and the receptive fields (local regions of the input image). Therefore:\n",
    "- We can rearrange the input volume into a matrix $\\mathbf{X}$, in which each column is a receptive field reshaped to a vector. The number of columns in this matrix is equal to the number of receptive fields in an image multiplied by the number of input images (batch size).\n",
    "- Next, we can reshape the convolution kernel into a vector. Note, however, that a convolutional layer has many kernels: the number of kernels is equal to the number of output channels. We can assemble these kernels into a matrix $\\mathbf{K}$, in which each row is one kernel (reshaped to a vector).\n",
    "- Once we have matrices $\\mathbf{X}$ and $\\mathbf{K}$, calculating the convolution is as simple as taking a matrix multiplication $\\mathbf{KX}$.\n",
    "\n",
    "A very important property of this implementation is that the convolution is expressed as a matrix multiplication. This is the same basic operation that is used in densely connected layers. This means that backpropagation equations from a simple MLPs also holds for this implementation of a convolutional layer. Matrix $\\mathbf{X}$ is the equivalent of the visible layer and matrix $\\mathbf{K}$ is the equivalent of the weight matrix.\n",
    "\n",
    "Important notes:\n",
    "- The rearrangements of the input volume into a matrix is implemented by ```im2col``` function.\n",
    "- During backpropagation we need to perform an operation inverse to ```im2col```. Specifically, we need to “collapse” deltas from a matrix into a volume with shape equal to the shape of the input. This inverse operation is implemented by ```col2im``` function.\n",
    "- Kernels in ```Conv2D``` are always kept as a matrix, where each row is a kernel for one output channel.\n",
    "- Convolution layer has no activation function (or, equivalently, has an identify activation function). Nonlinear activations are implemented by a separate layer (```Nonlinear``` layer). Therefore, we can disregard activation function (and its derivative) when implementing error backpropagation in ```Conv2D```.\n",
    "- We have a separate vector $\\mathbf{B}$ for biases. Our expression for convolution is therefore: $\\mathbf{KX} + \\mathbf{B}$. Note that biases are shared between receptive fields (vector $\\mathbf{B}$ is broadcasted to the shape of $\\mathbf{KX}$). See implementation of ```Dense``` layer for hints about proper implementation of biases in forward/backward pass.\n",
    "\n",
    "You can find more information about implementing convolution via ```im2col```/```col2im``` here:\n",
    "* http://cs231n.stanford.edu/slides/2016/winter1516_lecture11.pdf\n",
    "* http://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### im2col and col2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def col_indices(shape, kernel_size, strides):\n",
    "    kh, kw = kernel_size\n",
    "    sh, sw = strides\n",
    "    \n",
    "    n, c, h, w = shape\n",
    "    \n",
    "    col_extent = w - kw + 1\n",
    "    row_extent = h - kh + 1\n",
    "    \n",
    "    batch_idx = np.arange(n)[:, None, None] * c * h * w\n",
    "    \n",
    "    start_idx = np.arange(kh)[None, :, None] * w + np.arange(kw)\n",
    "    \n",
    "    depth_idx = h * w * np.arange(c)\n",
    "    start_idx = (depth_idx[None, :, None] + start_idx.ravel())\n",
    "    start_idx = start_idx.reshape((-1, kh, kw))\n",
    "    \n",
    "    offset_idx = np.arange(row_extent)[None, :, None] * w + np.arange(col_extent)\n",
    "    \n",
    "    idx = (batch_idx + \n",
    "           start_idx.ravel()[None, :, None] + \n",
    "           offset_idx[:, ::sh, ::sw].ravel())\n",
    "    \n",
    "    return idx\n",
    "\n",
    "def im2col(batch, kernel_size, strides, cached_indices=None):\n",
    "    if cached_indices is not None and batch.shape[0] == cached_indices.shape[0]:\n",
    "        idx = cached_indices\n",
    "    else:\n",
    "        idx = col_indices(batch.shape, kernel_size, strides)\n",
    "    \n",
    "    cols = np.take(batch, idx).transpose(1, 0, 2)\n",
    "    cols = np.reshape(cols, (cols.shape[0], -1))\n",
    "    return cols, idx\n",
    "\n",
    "def col2im(shape, cols, kernel_size, strides, cached_indices=None):\n",
    "    n, c, h, w = shape\n",
    "    kh, kw = kernel_size\n",
    "    \n",
    "    if cached_indices is not None and cols.shape == cached_indices.shape:\n",
    "        idx = cached_indices\n",
    "    else:\n",
    "        idx = col_indices(shape, kernel_size, strides).transpose(1, 0, 2).reshape(kh*kw*c, -1)\n",
    "    \n",
    "    im = zeros(*shape).reshape(-1)\n",
    "    for idxs, vals in zip(idx, cols):\n",
    "        im[idxs] += vals\n",
    "    \n",
    "    return im.reshape(*shape), idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D convolution\n",
    "\n",
    "Implement missing parts in the ```forward``` and ```backward``` methods.  Remember that the biases are kept in a separate vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 strides,\n",
    "                 padding,\n",
    "                 learning_rate,\n",
    "                 momentum,\n",
    "                 cache_col_indices=True):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        \n",
    "        kh, kw = self.kernel_size\n",
    "        self.klen = kh * kw * self.in_channels\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.cache_col_indices = cache_col_indices\n",
    "                    \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.K = np.random.normal(scale=np.sqrt(2.0 / self.klen),\n",
    "                                  size=(self.out_channels, self.klen)).astype(np.float32)\n",
    "        \n",
    "        self.B = zeros(self.out_channels, 1) + 1e-2 # Initialize biases to small positive values (for ReLU)\n",
    "        \n",
    "        self.padded_shape = None\n",
    "        self.cols         = None\n",
    "        \n",
    "        self.i2c_col_indeces = None\n",
    "        self.c2i_col_indeces = None\n",
    "        \n",
    "        self.MK = zeros(self.out_channels, self.klen)\n",
    "        self.MB = zeros(self.out_channels, 1)\n",
    "    \n",
    "    def pad(self, batch):\n",
    "        ph, pw = self.padding\n",
    "        if ph == 0 and pw == 0:\n",
    "            self.padded_shape = batch.shape\n",
    "            return batch\n",
    "\n",
    "        n, c, h, w = batch.shape        \n",
    "        batch_padded = zeros(n, c, h + 2 * ph, w + 2 * pw)\n",
    "        batch_padded[:, :, ph:(ph + h), pw:(pw + w)] = batch\n",
    "        \n",
    "        self.padded_shape = batch_padded.shape\n",
    "        \n",
    "        return batch_padded\n",
    "    \n",
    "    def unpad(self, deltas_padded):\n",
    "        ph, pw = self.padding\n",
    "        \n",
    "        deltas = deltas_padded[:, :, ph:-ph, :] if ph > 0 else deltas_padded\n",
    "        deltas = deltas[:, :, :, pw:-pw] if pw > 0 else deltas\n",
    "        \n",
    "        return deltas\n",
    "    \n",
    "    def output_size(self):\n",
    "        kh, kw = self.kernel_size\n",
    "        sh, sw = self.strides\n",
    "        _, _, padded_h, padded_w = self.padded_shape\n",
    "        \n",
    "        out_h = (padded_h - kh) / sh + 1\n",
    "        out_w = (padded_w - kw) / sw + 1\n",
    "        return int(out_h), int(out_w)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        batch_padded = self.pad(batch)\n",
    "        out_h, out_w = self.output_size()\n",
    "        \n",
    "        self.cols, i2c_col_indeces = im2col(batch_padded, self.kernel_size, self.strides, self.i2c_col_indeces)\n",
    "        if self.cache_col_indices:\n",
    "            self.i2c_col_indeces = i2c_col_indeces\n",
    "        \n",
    "        \n",
    "        raise NotImplementedError('Forward pass in Conv2D is not implemented.')\n",
    "        \n",
    "        # Calculate the convolution between the rearranged input volume and the convolution kernels.\n",
    "        # Add biases to the result. Store the final result in 'output' matrix.\n",
    "        #\n",
    "        # - the rearranged input volume is stored in 'self.cols'\n",
    "        # - the convolution kernels are stored in 'self.K'\n",
    "        # - the biases are stored in 'self.B'\n",
    "        \n",
    "        \n",
    "        # Using 'reshape' and 'transpose' NumPy operations reshape the 'output' matrix to the\n",
    "        # shape of the output volume.\n",
    "        #\n",
    "        # Currently 'output' matrix has the shape:\n",
    "        #   self.out_channels x (batch_size * out_h * out_w)\n",
    "        # where the order of elements in the second dimension is:\n",
    "        #   examples (batch_size) -> height (out_h) -> width (out_w).\n",
    "        #\n",
    "        # The output volume should have the shape:\n",
    "        #   batch_size x self.out_channels x out_h x out_w\n",
    "        \n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, deltas):\n",
    "        observations_count = self.padded_shape[0]\n",
    "        \n",
    "        deltas = deltas.transpose(1, 0, 2, 3)\n",
    "        deltas = deltas.reshape(self.out_channels, -1)\n",
    "        \n",
    "        \n",
    "        raise NotImplementedError('Backpropagation in Conv2D is not implemented.')\n",
    "        \n",
    "        # Calculate the deltas for the previous layer (i.e backpropagate the errors).\n",
    "        # Store the result in 'prev_deltas'.\n",
    "        #\n",
    "        # Deltas for the current layer are stored in `deltas`.\n",
    "        \n",
    "        # prev_deltas = ???\n",
    "        \n",
    "        \n",
    "        prev_deltas, c2i_col_indeces = col2im(self.padded_shape,\n",
    "                                              prev_deltas,\n",
    "                                              self.kernel_size,\n",
    "                                              self.strides,\n",
    "                                              self.c2i_col_indeces)\n",
    "        if self.cache_col_indices:\n",
    "            self.c2i_col_indeces = c2i_col_indeces        \n",
    "        \n",
    "        prev_deltas = self.unpad(prev_deltas)\n",
    "        \n",
    "        \n",
    "        raise NotImplementedError('Gradient calculation in Conv2D is not implemented.')\n",
    "        \n",
    "        # Calculate the gradient for the kernels and biases. Store the result\n",
    "        # in `grad_k` and `grad_b`, respectively.\n",
    "        \n",
    "        # grad_k = ???\n",
    "        # grad_b = ???\n",
    "        \n",
    "        \n",
    "        grad_b = np.expand_dims(grad_b, axis=1)\n",
    "        \n",
    "        self.MK = self.momentum * self.MK - \\\n",
    "                  self.learning_rate / observations_count * grad_k\n",
    "        self.MB = self.momentum * self.MB - \\\n",
    "                  self.learning_rate / observations_count * grad_b\n",
    "        \n",
    "        self.K += self.MK\n",
    "        self.B += self.MB\n",
    "        \n",
    "        return prev_deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(mlp, batch):\n",
    "    for layer in mlp:\n",
    "        batch = layer.forward(batch)\n",
    "        \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_backpropagate(mlp, deltas):\n",
    "    for layer in reversed(mlp):\n",
    "        deltas = layer.backward(deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_mlp(mlp, dataset, labels, batch_size):\n",
    "    batches_limit = dataset.shape[0] // batch_size\n",
    "    \n",
    "    batched_data = chunks(dataset, batch_size)\n",
    "    batched_labels = chunks(labels, batch_size)\n",
    "    \n",
    "    for batch_idx, (batch, batch_labels) in enumerate(zip(batched_data, batched_labels)):\n",
    "\n",
    "        y = forward_pass(mlp, batch)\n",
    "        \n",
    "        deltas = y - batch_labels\n",
    "        error_backpropagate(mlp[0:-1], deltas)\n",
    "        \n",
    "        if batch_idx % round(batches_limit / 40) == 0: print(\"#\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST digits classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(mlp, batch):\n",
    "    probabilities = np.concatenate([forward_pass(mlp, mini_batch) for mini_batch in chunks(batch, 128)])\n",
    "    return np.argmax(probabilities, axis=1)\n",
    "\n",
    "def run_training(mlp, train_set, train_labels,\n",
    "                 validation_set, validation_labels,\n",
    "                 batch_size, epochs_count):\n",
    "    \n",
    "    for epoch in range(epochs_count):\n",
    "        print(\"Epoch {}:\".format(epoch+1),  end=\"\\t\")\n",
    "\n",
    "        if epoch == 5:\n",
    "            for layer in mlp:\n",
    "                layer.momentum = 0.9\n",
    "                \n",
    "        start_time = time.time()\n",
    "        train_mlp(mlp, train_set, train_labels, batch_size)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        predictions = classify(mlp, validation_set)\n",
    "        accuracy = 100.0 * np.sum(predictions == validation_labels) / predictions.shape[0]\n",
    "        print(\"\\telapsed: {0:>2.2f}s, accuracy: {1:>2.2f}\".format(elapsed, accuracy))\n",
    "\n",
    "    print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "DATASET_SIZE = 10000 # 60000 for whole dataset\n",
    "DIGIT_SIZE = 28\n",
    "\n",
    "##### Train set #####\n",
    "\n",
    "mnist_train_images = mnist.train_images().astype(np.float32) / 255.0\n",
    "mnist_train_labels = mnist.train_labels()\n",
    "\n",
    "order = np.random.permutation(len(mnist_train_images))\n",
    "mnist_train_images = mnist_train_images[order]\n",
    "mnist_train_labels = mnist_train_labels[order]\n",
    "\n",
    "mnist_train_images = mnist_train_images[:DATASET_SIZE]\n",
    "mnist_train_images = mnist_train_images.reshape(-1, 1, DIGIT_SIZE, DIGIT_SIZE)\n",
    "\n",
    "mnist_train_labels = mnist_train_labels[:DATASET_SIZE]\n",
    "mnist_train_labels = one_hot_encode(mnist_train_labels)\n",
    "\n",
    "##### Test set #####\n",
    "\n",
    "mnist_test_images = mnist.test_images().astype(np.float32) / 255.0\n",
    "mnist_test_images = mnist_test_images.reshape(-1, 1, DIGIT_SIZE, DIGIT_SIZE)\n",
    "\n",
    "mnist_test_labels = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS_COUNT = 50\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "mlp = [\n",
    "    Conv2D(\n",
    "        in_channels=1,\n",
    "        out_channels=2,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        padding=(1, 1),\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        momentum=MOMENTUM),\n",
    "    Nonlinear(relu, relu_derivative),\n",
    "    Conv2D(\n",
    "        in_channels=2,\n",
    "        out_channels=2,\n",
    "        kernel_size=(2, 2),\n",
    "        strides=(2, 2),\n",
    "        padding=(0, 0),\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        momentum=MOMENTUM),\n",
    "    Nonlinear(relu, relu_derivative),\n",
    "    Conv2D(\n",
    "        in_channels=2,\n",
    "        out_channels=4,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        padding=(1, 1),\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        momentum=MOMENTUM),\n",
    "    Nonlinear(relu, relu_derivative),\n",
    "    Conv2D(\n",
    "        in_channels=4,\n",
    "        out_channels=4,\n",
    "        kernel_size=(2, 2),\n",
    "        strides=(2, 2),\n",
    "        padding=(0, 0),\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        momentum=MOMENTUM),\n",
    "    Nonlinear(relu, relu_derivative),\n",
    "    Conv2D(\n",
    "        in_channels=4,\n",
    "        out_channels=32,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        padding=(1, 1),\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        momentum=MOMENTUM),\n",
    "    Nonlinear(relu, relu_derivative),\n",
    "    Dense(\n",
    "        visible_size=32 * 7 * 7,\n",
    "        hidden_size=10,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        momentum=MOMENTUM),\n",
    "    Nonlinear(softmax, None)\n",
    "]\n",
    "\n",
    "run_training(mlp,\n",
    "             mnist_train_images, mnist_train_labels,\n",
    "             mnist_test_images, mnist_test_labels,\n",
    "             BATCH_SIZE, EPOCHS_COUNT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
